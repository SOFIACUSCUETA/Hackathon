{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_lgbm(y_pred, data):\n",
    "    y_true = np.array(data.get_label())\n",
    "    score = np.sqrt(np.mean(np.power(np.log1p(y_true) - np.log1p(y_pred), 2)))\n",
    "    return 'rmsle', score, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_casas= pd.read_csv('house_train_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicciones solicitadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_casas_test = pd.read_csv('houses_test_raw.csv')\n",
    "Data_casas_test['SalePrice'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_casas.describe()\n",
    "Data_casas.info()\n",
    "Data_casas.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la sumatoria de nulos, decidimos eliminar: PoolQC,MiscFeature,Alley,Fence,FireplaceQu y LotFrontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_casas = Data_casas.drop(columns=['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Data_casas['SalePrice']\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda de correlación entre variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = Data_casas.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "#Sale Price correlaciona mejor con OverallQual y con GrLivArea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de correlación de Sale Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 # Número de variables.\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(Data_casas[cols].values.T)\n",
    "sns.set(font_scale = 1.25)\n",
    "hm = sns.heatmap(cm, cbar = True, annot = True, square = True, fmt = '.2f', annot_kws = {'size': 10}, yticklabels = cols.values, xticklabels = cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista de correlaciones ordenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = Data_casas.corr()\n",
    "corr[['SalePrice']].sort_values(by = 'SalePrice',ascending = False)\\\n",
    ".style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Según el analisis, nos quedaremos solo con las siguientes variables:\n",
    "x = Data_casas[['GrLivArea','BsmtFinSF1','TotalBsmtSF','OverallQual','LotArea','OverallCond','YearBuilt','GarageArea','YearRemodAdd','GarageYrBlt','SaleType','FullBath','GarageCars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con esto, la idea es pasar a numerica la variable SaleType:\n",
    "x = pd.get_dummies(x, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = { \n",
    "    'boosting': 'gbdt', #gbdt\n",
    "    'objective': 'regression',\n",
    "    'num_leaves': 10,\n",
    "    'learnnig_rage': 0.05,\n",
    "    'max_bin': 255,\n",
    "    'metric': 'custom',\n",
    "    'verbose': -100,\n",
    "    'num_iterations': 5000,\n",
    "    'max_depth' : 7,\n",
    "    'min_data_in_leaf': 5,\n",
    "    'feature_fraction': 0.40,\n",
    "    'bagging_freq': 100,\n",
    "    'extra_trees' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento de varios modelos para simular el Cross-Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,500):\n",
    "    #Separamos en train / test:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state = i)\n",
    "    #Adecuamos los datos para el modelo:\n",
    "    lgb_train = lgb.Dataset(x_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n",
    "    \n",
    "    #Entrenamos el modelo:\n",
    "    modelo = lgb.train(parametros,\n",
    "                     train_set=lgb_train,\n",
    "                     valid_sets=lgb_eval,\n",
    "                     num_boost_round=500,\n",
    "                     early_stopping_rounds=3000,\n",
    "                     feval=rmsle_lgbm)\n",
    "    \n",
    "    # predicciones:\n",
    "    #y_pred = modelo.predict(x_test)\n",
    "\n",
    "    # Hacemos backup del dataset de test:\n",
    "    Data_casas_test_aux = Data_casas_test\n",
    "\n",
    "    # Se realizan los mismos cambios de variables que el dataset de entrenamiento:\n",
    "    Data_casas_test_aux = Data_casas_test_aux.drop(columns=['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage'])\n",
    "    Data_casas_test_aux = Data_casas_test_aux[['GrLivArea','BsmtFinSF1','TotalBsmtSF','OverallQual','LotArea','OverallCond','YearBuilt','GarageArea','YearRemodAdd','GarageYrBlt','SaleType','FullBath','GarageCars']]\n",
    "    \n",
    "    #Con esto, la idea es pasar a numerica la variable SaleType:\n",
    "    Data_casas_test_aux = pd.get_dummies(Data_casas_test_aux, drop_first = True)\n",
    "    \n",
    "    # Predicciones de las casas desconocidas:\n",
    "    pred_nuevas_casas = modelo.predict(Data_casas_test_aux)\n",
    "    \n",
    "    # Empezamos a acumular el precio sobre la columna SalePrice\n",
    "    Data_casas_test['SalePrice'] = Data_casas_test['SalePrice'] + pred_nuevas_casas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuando se generaron los 500 modelos, y se predijo el precio con todos, se promedia:\n",
    "Data_casas_test['SalePrice'] = Data_casas_test['SalePrice']/500\n",
    "Data_casas_test.to_csv('pred_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6ec33a374fe02d31de8c02fe0afc824eee7c8d116d301867a330e491c8e2dc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
